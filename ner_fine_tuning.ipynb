{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae80027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 1.14MB [00:00, 54.0MB/s]                  \n",
      "Downloading data: 200kB [00:00, 50.7MB/s]                    \n",
      "Downloading data: 206kB [00:00, 213MB/s]                     \n",
      "Generating train split: 100%|██████████| 5433/5433 [00:00<00:00, 30240.00 examples/s]\n",
      "Generating validation split: 100%|██████████| 924/924 [00:00<00:00, 30673.27 examples/s]\n",
      "Generating test split: 100%|██████████| 941/941 [00:00<00:00, 29823.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#fetch ncbi disease dataset and split to train and validation\n",
    "dataset = load_dataset(\"ncbi_disease\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mock synonym foor augmentation, could add a lot more\n",
    "synonyms = {\n",
    "    \"glioblastoma\": [\"GBM\", \"glioblastoma multiforme\"],\n",
    "    \"tp53\": [\"tumor protein p53\", \"p53 gene\"],\n",
    "}\n",
    "\n",
    "def augment_sentence(tokens):\n",
    "    augmented = []\n",
    "    for token in tokens:\n",
    "        key = token.lower()\n",
    "        if key in synonyms and random.random() < 0.3:  # 30% chance to replace\n",
    "            augmented.append(random.choice(synonyms[key]))\n",
    "        else:\n",
    "            augmented.append(token)\n",
    "    return augmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5433/5433 [00:00<00:00, 31774.54 examples/s]\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at d4data/biomedical-ner-all and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([84]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([84, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 10866/10866 [00:01<00:00, 6304.94 examples/s]\n",
      "Map: 100%|██████████| 941/941 [00:00<00:00, 5988.98 examples/s]\n",
      "/var/folders/3j/tvgszzpn1yd6rc09xm68js3hf7bdz7/T/ipykernel_52048/2847648006.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='13590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   70/13590 00:04 < 14:29, 15.54 it/s, Epoch 0.05/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Label mappings (adapt as per dataset)\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-DISEASE\": 1,\n",
    "    \"I-DISEASE\": 2,\n",
    "    # add other labels if needed\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Load dataset from HF hub (NCBI Disease)\n",
    "raw_train = load_dataset(\"ncbi_disease\", split=\"train\")\n",
    "raw_val = load_dataset(\"ncbi_disease\", split=\"test\")\n",
    "\n",
    "# Simple synonym dictionary for augmentation\n",
    "synonyms = {\n",
    "    \"glioblastoma\": [\"GBM\", \"glioblastoma multiforme\"],\n",
    "    \"tp53\": [\"tumor protein p53\", \"p53 gene\"],\n",
    "}\n",
    "\n",
    "def augment_example(example):\n",
    "    tokens = example[\"tokens\"]\n",
    "    augmented_tokens = []\n",
    "    for t in tokens:\n",
    "        key = t.lower()\n",
    "        if key in synonyms and random.random() < 0.3:\n",
    "            augmented_tokens.append(random.choice(synonyms[key]))\n",
    "        else:\n",
    "            augmented_tokens.append(t)\n",
    "    example[\"tokens\"] = augmented_tokens\n",
    "    return example\n",
    "\n",
    "# Augment training data (double data size)\n",
    "augmented_train = raw_train.map(augment_example)\n",
    "\n",
    "# Combine original + augmented\n",
    "train_dataset = concatenate_datasets([raw_train, augmented_train])\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Tokenize and align labels function here (same as before)\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                current_label = label[word_idx]\n",
    "                if current_label % 2 == 1:\n",
    "                    label_ids.append(current_label + 1)\n",
    "                else:\n",
    "                    label_ids.append(current_label)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = raw_val.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_finetuned_rare_disease\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model(\"./ner_finetuned_rare_disease_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
